
[epoch:  36/100, batch:    72/  463, ite: 2004] train loss: 0.063560, tar: 0.005772
l0: 0.005109, l1: 0.005143, l2: 0.005395, l3: 0.006044, l4: 0.007300, l5: 0.009253, l6: 0.015919

[epoch:  36/100, batch:    80/  463, ite: 2005] train loss: 0.061680, tar: 0.005639
l0: 0.006755, l1: 0.006731, l2: 0.007046, l3: 0.007510, l4: 0.008299, l5: 0.010631, l6: 0.020647

[epoch:  36/100, batch:    88/  463, ite: 2006] train loss: 0.062670, tar: 0.005825
l0: 0.006195, l1: 0.006269, l2: 0.006907, l3: 0.007432, l4: 0.009689, l5: 0.018580, l6: 0.045879

[epoch:  36/100, batch:    96/  463, ite: 2007] train loss: 0.068139, tar: 0.005878
l0: 0.005654, l1: 0.005803, l2: 0.005979, l3: 0.006683, l4: 0.008877, l5: 0.011929, l6: 0.022465

[epoch:  36/100, batch:   104/  463, ite: 2008] train loss: 0.068045, tar: 0.005850
l0: 0.004358, l1: 0.004414, l2: 0.004852, l3: 0.005653, l4: 0.007194, l5: 0.012666, l6: 0.022688

[epoch:  36/100, batch:   112/  463, ite: 2009] train loss: 0.067354, tar: 0.005684
l0: 0.005190, l1: 0.005213, l2: 0.005342, l3: 0.006072, l4: 0.006942, l5: 0.010183, l6: 0.021781

[epoch:  36/100, batch:   120/  463, ite: 2010] train loss: 0.066691, tar: 0.005635
l0: 0.004541, l1: 0.004536, l2: 0.004661, l3: 0.005025, l4: 0.005908, l5: 0.008033, l6: 0.014819

[epoch:  36/100, batch:   128/  463, ite: 2011] train loss: 0.064949, tar: 0.005535
l0: 0.003884, l1: 0.003881, l2: 0.004127, l3: 0.004689, l4: 0.005720, l5: 0.008140, l6: 0.014778

[epoch:  36/100, batch:   136/  463, ite: 2012] train loss: 0.063304, tar: 0.005398
l0: 0.005176, l1: 0.005260, l2: 0.005471, l3: 0.006290, l4: 0.008132, l5: 0.010976, l6: 0.019097

[epoch:  36/100, batch:   144/  463, ite: 2013] train loss: 0.063081, tar: 0.005381

